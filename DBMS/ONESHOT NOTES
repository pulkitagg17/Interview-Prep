LEC-1

DATA
-raw, unorganised facts and details like text,fig, symbols
-data does not carry any specific purpose and has no significance by itself
-data is raw information

TYPES OF DATA
1. Quantitative - numerical(weight,volume,price)
2. Qualitative - descriptive (color, texture, taste)

INFORMATION
-information is processed data that has meaning and purpose
-information is data that has been organized or structured in a way that it can be understood and used
-info is extracted from data by analyzing, interpreting, and presenting it in a meaningful context

DATA VS INFORMATION
- Data is raw facts, while information is processed data.
- unorganised vs organised
- Data has no meaning, while information has meaning and purpose(when analyzed and interpreted)
- Data does not depend on information, but information depends on data.
- Data isnt sufficient for decision making, but you can make decisions based on information.

DATABASE - an electronic place or system where data is stored in a way that it can be easily accessed, managed, and updated.
- to make real use of it we need DBMS

DBMS (Database Management System)
- it is a collection of interraleted data(database) and a set of programs to access those data.
-primary goal is to provide a way to store and retrieve database info.
- DBMS is the database itself, along with all software and functionality(oerations include CRUD)

DBMS vs FILE SYSTEM
- data redundancy and inconsistency
- difficulty in accessing data
- intergrity problems
- atomicity problems
- concurrent-access anomalies
- security problems
above are disadv of file and adv of DBMS

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC-2(DBMS ARCHITECTURE)

VIEWS OF DATA(THREE-SCHEMA ARCH)
- major purpose of DBMS - user abstraction(hide how data is stored and maintained)
-abstraction is applied through several levels for simplify user interaction
- the main reason for three level is to unable multi user to access the same data with personalised views while storing it only once

THREE LEVELS
1.  Physical Level/Internal Level (Goal- we must define algo that stores data  efficiently)
  - lowest level - how data is stored - low level DS is used
  - it has Physical schema ->describes physical storage strc of DB
  - Talks about-> storage allocation(N-ary Tree), data compression, encryption.

2.  Logical Level/ Conceputal Level(Goal- ease of use)
  - Conceputal schema -> descibes the design of db and what data are stored and what relationships exists among them(data).
  - user view is independent of physical storage
  - DBA use this level of abstraction to define the structure of the database

3.  View Level/ External Level(Goal- user interaction)
  - high level abstraction-> to simplify user interaction with system by providing diff view
  - Each view schema describes a part of interested user group and hides remaining data
  - View schema -> describes how data is presented to the user
  - at this level - subschema exists which is subset of conceptual schema(they define diff  views of database)
  - view also provides security mechanism -> prevent user from accessing certain parts of DB


INSTANCES AND SCHEMAS
- Instances - collection of info stored in DB at a particular moment
- Schema - the overall design of DB
         - it is structural description of data(doesnt change frequently, data may)
         - DB schema corrosponds to variable decraltions(along with type) in prog.
         - it is a blueprint of the database
         - Three Schemas - Physical, Logical, View(called subschema)
         - logical schema is the most important as it defines the structure of the database
         - physical schema is the least important as it deals with how data is stored and should not affect logical schema or design


DATA MODELS(ER model, Relational model, Object-oriented model, object-relational model)
- provides a way to describe the design of DB at logical level
- underlying DB struc -> is Data models -> a collections of conceputal tools for describing data, data relationships, data semantics and consistency constraints.


DATABASE LANGUAGES
- DDL - Data Definition Language
      - used to define the database structure and schema
      - commands include CREATE, ALTER, DROP
      - it defines the tables, fields, data types, and relationships

- DML - Data Manipulation Language
      - used to manipulate data within the database
      - commands include SELECT, INSERT, UPDATE, DELETE
      - it allows users to retrieve, insert, update, and delete data

- DCL - Data Control Language
      - used to control access to data in the database
      - commands include GRANT, REVOKE
      - it allows administrators to define user permissions and roles

- TCL - Transaction Control Language
      - used to manage transactions in the database
      - commands include COMMIT, ROLLBACK, SAVEPOINT
      - it ensures data integrity by allowing users to group multiple operations into a single transaction

- SQL - Structured Query Language
      - a standard language for managing and manipulating relational databases
      - it includes DDL, DML, DCL, and TCL commands
      - SQL is used to create, modify, query, and control access to databases
      - it is the most widely used language for interacting with relational databases
      - SQL is a declarative language, meaning users specify what they want to do with the data, not how to do it

HOW DB ACCESSED FROM APPLICATION PROGRAMS?
- apps written in host language like C/C++, Java, Python, etc. interacts with DB
- DBMS provides APIs or libraries that provide to send DDL/DML commands to DB and retrieve results
- ODBC - microsoft C
- JDBC - java
- Python - sqlite3, psycopg2, pymysql

DATABASE ADMINISTRATOR(DBA)
- DBA has central control of both data and programs that access those data
- functions - Schema defination
            - storage struc and access method
            - schema and physical orginazation modifications
            - authorization control
            - routine maintaince -> periodic backups, security patches, any upgrades


DBMS APPLICATION ARCHITECTURE(client machine + remote db user + server machines)

T1 Architecture 
- client,server and db all present on same machine

T2 Architecture(2 componets)
- C1 - user + application(client machine invokes DB system functionality at server end through query language)
- C2 - db system
-API like ODBS, JBDC are used to communicate btw these two

T3 Architecture(3 Components) -> best for WWW application
- Clinet machine - only frontend - no db calls
- clinet machine - comm - app server - comm - db system(To access data)
- business logic - app server

adv - scalabilty(due to distribution), data intergrity(app server acts as middle layer), security(client cant directly access DB)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 3(ER Model)

ER Model
- high level data model - real world perception - collection of entities and relationships(basic objects)
-ER model grapgically known as ER diagram -> blueprint of DB

ENTITY(thing or object which is distinguishable from all other objs)
- it has physical existence, can be uniquely identified(student_id in college)
- strong entity - can be uniquely identified - primary key
- weak entity - cant be - depends on some other strong entity 
              - loan->strong entity, payment -> weak, as instalments are sequential no counter can be generated for each loan
- weak depends on strong for existence

ENTITY SET 
- set of entities of same type shares same properties or attributes
- student is an entity set

ATTRIBUTES 
- entity is rep by set of attributes
- each entity got value for each attributes(set of permitted values -> domain or value set)
- eg student entity attributes -> student_id, name, class, course etc

TYPES OF ATTRIBUTES

simple - cnt be divided further(customers acc number)
composite - can be divided further(address-> city, state, street etc)
single-valued - only one value attribute(student_id)
multi-valued - more than one value attribute(phoen no, nominee name etc, limit may be applied)
Derived - values are derived from other realted attribute(Age)
null-valued - no value of attribute(middle name) -> may also indicate unknown(employee salary)

RELATIONSHIPS 
- association among two or more entities
- strong relationship - two independent entities
- weak relationship - btw weak and strong(or with its owner) entity(loan-payment)
- Degree of relationship(no of entities participation) -> unary(employee manages emp), binary(common, student - courses), ternary(emp work on branch - emp work on job)

RELATIONSHIPS CONSTRAINTS 
Mapping Cardinaltiy/ Cardinaltiy Ratio -> no of entities to which another entity can be associated via relationship
- one to one(citizen has adhar card)
- one to many(citizen has vehicle)
- many to one(course taken by professor)
- many to many(customer by products, student attend courses)

Participation constraints(aka minimum Cardinaltiy constraints)
- partial participation -> not all entities are involved in relaionship Instances
- total participation -> each entity must be invovled at least one relationship Instances
- eg - cust takes loan, loan has total parti cause it cant exists without cust but cust has partial(weak entity has total participation but strong may not).

ER notation - done in notebook

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 4(ER FEATURES)

-when complexity increase it is better to use extended ER FEATURES

SPECIALISATION(splitting up the entity set into further sub-entity on on basis of functionalites , specialities and features - TOP DOWN APPROACH)
- we may require to subgroup an entity - sub entity
- Person(superclass) entity set -> customer, student, emp etc(these are subclass)
- we have "is-a" relationship between super and sub
- depicted by triangle

why specialisation?
- to group such entites whose attributes may only applicable to few entites of parent sentity set and DB designer can show their distnctive feature of sub entites

GENERALISATION(reverse of specialisation)
- DB designer may bound overlapping two entites -> designer may consider making a generalised set.
- bottom up approach-> "is-a" is also used here
-> why? makes DB more refined and simpler, commmon attributes are not repeated

ATTRIBUTES INHERITANCE
- BOTH GENERALISATION AND SPECIALISATION  have it
- attributes of higher level entity set are inherted by lower entity sets 
- eg - cust and emp inherts attribute person 

PARTICIPATION INHERITANCE - if parent entity set participate in a relationship then child will too!

AGGREGATION
- to show relationship among relationships
- abstraction is applied to treat relationship as higher-level entites-> call it abstract entity
- avoid redundancy by aggregating relationship as entity set itself.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
other lecture were about making er model(we will get to it)
LEC - 7(Relational Model -> RDBMS -> oracle, IBM, mySql,MS Access)

- organises data in form of tables, collections of table with unique name
- row(tuple-> a single row of table representing a single data in unique table) rep a relaionship among set of values
- Column rep attribute of relation, each attri -> has permitted value-> domain of attribute
- Relational Schema - define desgin and struc of relation(Contain names of relation and all column/attri)
- Degree of Table - no of attri/column
- Cardinality - total no of tuples
- Relational key - set of attri which can uniquely identify each tuple
- Imp properties of R-Table - relation name/attri name/ tuple -> distinct/unique
                            - value -> atomic(cant be broken down)
                            - sequence of row and col ->had no significance
                            - Table must follow intergrty constraints



RELATIONAL MODEL KEYS
super key     - uniquly identify each tuple
Candidate Key - min subset of super key -> no redudant attri , cant be null
Primary key   - selected out of CK(least no of attri)
Alternate key - All CK except PK
Forgein key   - create relation btw two table
              - r1 to r2 referencing(r1 may include among its attri PK of another relation r2)
              - r1 - referecning(child) of FK depedency and r2 - referenced(parent) relation
              - it helps to cross refercnce btw two diff table
Composite key - PK formed using atleast 2 attri
Compound key  - PK which is formed using 2 FK
Surrogate Key - synthetic PK, generated auto by DB - usually int, maybe used as PK


INTERGRITY CONSTRAINTS
- CRUD operations must be done with intergrity-> always consistent and donot corrupt
- Domain constraints - restricts value in relation, in domain
                     - restricts the data type of every attri 
- Entity constraints - every relation should have PK!=NULL


REFERENTIAL CONSTRAINTS
- specified btw two realtion(maintain consistency)
- requires-> value in specified attri of any tuple should reflect in atleast one tuple of refercncing realtion
-child table should have values of master table's refercnce key(PK or UK) or must be NULL


KEY CONSTRAINTS
- NOT NULL - should have value
- UNIQUE - values in column are diff
- DEFAULT - use to set default value if not inserted
- CHECK  - intergrity constraints-> it keeps check of integrity before and after CRUD operation
- PRIMARY KEY - attri or set of attri that unique identifies each entity in set(must be unique and not null)
- FORGEIN KEY - t1 primary should be fk of another table entity set-> this constraints help to maintain the connection.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 8 skip for now
lec - 9 and 10 sql and queries

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 11(NORMALISATION - step toward DB optimization, why? -> to avoid reduncancy)

Functional Dependency(FD)
- relation btw primary key attri with other attri
- X(Determinant) -> Y(Dependent)

Types of FD
1. Trivial FD - A->B (B is subset of A), also A->A and B->B
2. Non-Trival - B is not the subset of A (A intersection B = NULL)

Rules of FD(Armstrong's Axioms)
1. Reflexive - if A is set of attri and B is subset of A, A->B holds
2. Augmentation - A->B holds, AX->BX also holds(adding X attrin wont change anything)
3. Transitivity - A->B, B->C then A->C (holds)

what if we have redundant data?
- insert, delete, update anomalies

ANOMALIES - means abnormalities
1. Insertion Anomalies - when data(attri) can be inserted without presence of other data
2. Deletion Anomalies - situation where the deletion of data results in unintended loss of some other imp data
3. Updation Anomalies(or modification anomalies) - single updation of data needs multiple rows data updation
                                                 - due to many data updation -> data inconsistency arise  
-Due to these anomalies - DB size inc, DB perfomance des 

What is Normalisation?
-minimise reduncancy, eliminate undesirable characteristics like insertion, update and deletion anomalies
- divided composite attri to individual attri or larger table into smaller and links them using relationship
- normal form is used to reduce reduncancy from database table

TYPES OF NORMAL FORMS
1NF - every relation must have atomic value
    - relation must not have multivalued attri

2NF - must be 1NF
    - no partial Dependency - all non prime attri must be fully dependent on PK
                            - non prime attribute can not depend on part of PK

3NF - must be 2NF
    - no transitivity dependency exists - non prime attri should not find a non-prime attri

BCNF(boyce-Codd NF) - relation must be 3NF
                    - FD - A->B, A must be super key
                    - we must not derive prime attribute from any prime or non-rpime attri

ADV of Normalisation - minimise data reduncancy
                     - greater overall database organisation
                     - data consistency is maintained in DB.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 12

Transaction
- a unit of work done against the DB in a logical sequence(seq is imp)
- to put it in good words - it is a logical unit of work that contains one or more sql statements, result of all statement either to be completed successfully or rolledback to its previous form


ACID PROPERTIES(to ensure the integrity of DB)
- Atomicity - either all the success or none
- Consistency - integrity constraints must be maintained before and after a transcation
- Isolation - even though multiple transaction may execute concurrently, it is guaranted that either one of them will first finish then each other - hence a transaction is isolated and is unaware of the other one.
- Durability - after transaction completion, changes should persist in db, even in system failures.


TRANSACTION STATES
Active state - very first state, read and write op are performed, no error->Partially, error->Failed state
Partially Committed state - T executed(stored in buffer), if changes made permanent-> commited, error/failure-> failed state
Committed state - changes->saved->permanet->cant be rolledback->new consistent state achieved
Failed state - T executes-> failure occurs-> impossible to executes-> failed state
Aborted state - T reaches failed state-> changes are rolled back -> DB achieves Prior state
Terminated state - either commited or aborted
 
	partial -> committed 
       /  |                \
      /   |                 \
active    |                  Terminated
     \    |                  /
      \   |                 /
       failed -----> aborted  

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LEC - 13(Implementing Atomicity and Duralbility(Recovery mechanism Components) in Transaction)

SHADOW COPY SCHEME
- create a copy(shadow copy) of db
- maintain a db-pointer at disk-> at any instance point at DB, if want to update DB(lets say T)-> create a full copy of DB.
- futher updates are done on new copy of dB -> leaving original intact
- if T has to aborted -> copy is deleted -> orignal stays unaffected
- if T is succuss -> it is committed

- OS make sure all pages of new copy are written on disk
- DB system update db-pointer -> to new copy point
- new copy = curr copy -> old one deleted
- T is COMMITED 

ATOMICITY - if T fails -> old copy delete, T can be aborted -> delete new copy, reflected all updates or none
DURABILITY - if system fail before update -> restarts -> read db-pointer, see original content -> no effects will be visible(T is succuss only when db-pointer updates)
           - if system fails after db-pointer update -> since all have been updated -> system restarts -> read new copy of db

**shortNote** - implementaion depends on db-pointer writing, Luckily disk system provide atomic update to disk block or sector-> so db-pointer lies entirely in a single sector-> by storing db-pointer in beginning of disk.

inefficient -> entire db is copied for every transaction.

LOG-BASED RECOVERY METHODS
- it is a sequence of record-> maintained in stable storage-> if failure-> recoverd from that
- process of storing log should be done before actual transaction is applied in db -> any op performed -> recorded in log
- STABLE STORAGE -> techno of computer storage-> guarantees atomicity at any given write op, also robut against some hardware and power failures.

DEFERRED DB MODIFICATION(db only updated if transaction fully commited)
- it ensures atomicity by recording all op in log -> only if transaction is fully comm
- if T fails/aborted, info in log is ignored
- if T success, records associated in log file are used to deferred write execution
- if T fails while updating - we redo it

IMMEDIATE DB MODIFICATION
- The database system updates the actual database on disk immediately (or fairly soon) even before the transaction has committed.
- DB modification written by active T is called uncommitted modifications
- failure and succuss point discussed above.
- update only take place when log records in stable storage
- failure handling - failure befire comm/abort-> old value restored
                   - if T comm and sys crash -> new values field is used to redo T having commit logs in logs.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


LEC - 14(INDEX - a DS used to locate and access data quickly)

- Indexing -> use to optimize performance of db by minimising no. of disk access
- speed up operation -> with read op like select and where clause
- Search Key -> contains copy of PK or CK of table
- Data Ref. -> pointer holding add of disk block -> value of key is stored
- Indexing is optional and secondary mean to access tuple
- Index file is always sorted

INDEXING METHODS
1. Primary Indexing(Clustering Index)
    - if data file is sequential in order -> a Primary Index(can be PK or non-PK) is key whose search key also defines the sequential order of file.
    - primary index != PK
    DENSE AND SPARSE INDICES
    1. Dense Index  
        - it contains record for every search key value in data file
        - index record contains search-key value and pointer to first data record with search-key value, rest of record are stored sequentially after first record
        - it needs more space to store idx records itself.
    
    2. Sparse Index
        - idx record appears for only some of search-key values
        - sparse idx help to resolve dense indexing-> in this method, a range of index columns stores the same data block address, when data needs to be retrieved, the block address will be fetched.
    
    Primary Indexing can be Based on
    1. Key Attri
        - data files stored wrt PK-> Pk will be used as Search-key in idx
        - Sparse idx will be formed-> no. of entries in idx file = no. of blocks in datafile
    
    2. Non-Key Attri
        - data stored wrt non-key attri
        - no. of entries in idx = unique non-key attri in datafile
        - this is dense idx-> hence all unique key have an entry in idx file
    
    3. Multi-level Indexing
        - index with two or more levels
        - if single level become large->binary search would take more time
    
2. Secondary Index(Non-Clustering idx)
    - datafile unsorted -> Primary Indexing not possible
    - can be done on key or non-key attri
    - called secondary cause one indexing is applied normally
    - no. of entries = no. of records in datafile
    - its an example of dense index

ADV - faster access and retrieval, I/O is less
DIS - add space, idxing dec performance - insert,delete,update

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


LEC - 15(NO-SQL)
- non-tabular db, main types-> key-value, wide-column, graph
- provide flexible schemas(adjust dynamically) and scale easily with large data & high load
- open source and capable of horizantal scaling

HISTORY OF NoSql - rise of developers, unstructed data became trend as storage capacity rose
                 - unstructued data gave them alot of flexibility
                 - cloud computing became trend, distributing data accross multiple servers.

No-Sql ADV
- Flexible Schema
- Horizontal Scaling(also known as scale-out) - eff with non-rel db-> db collected are self-contained and not coupled relationally - no join op, achieved thorugh Sharding or Replica-Set
- High Availability - due to auto replication
- Easy Insert and Read Op - but diff delete & update
- Caching
- Use case in Cloud Comp.

when to use? - Fast-paced agile dev
             - Semi-stru data and huge vol
             - Req for scale-out arch
             - morden services-> realtime streaming and micro-services

No-Sql Misconception
- relationship data better stores in RDBMS -> well it is easy in NOSQL
- No-Sql dont support ACID - it do


Types of NoSql Data Models
1. Key-Value Stores(Amazon DynamoDB, MongoDB, Oracle No-Sql)
    - attri named key and value(only two columns)
    - use case -> user profile, shopping carts etc
    - use compact, eff idx struc to quickly find a key(ideal for constant time finding)
    - optimal use case -> real time random data access ->gaming, finance
                       -> Caching Mech for frquent data access
                       -> Application is designed on simple key-value queries

2. Column-Oriented/ Columnar/ C-Store/ Wide-Column(Cassandra, RedShift, Snowflake)
    - data is stored as each row of column will be next to other rows from that sawe column
    - use case -> analytics

3. Document Based Stores(MongoDB, COuchDB)
    - DB stores data in doc similar to JSON, each doc contains pair of field and values -> values can be string,int,bool,array,objects
    - use case - ecomm, trading plat, mobile devlopment accross industries

4. Graph Based Stores
    - focus in relationship btw data elements, each is stored as node, connection btw ele is called links or relationships
    - in it, connections are first-class elements of db
    - optimised to capture and search connection btw data ele, overcoming overhead, we usually get from joins
    - use case -> fraud detection, social netwrok, knowlege graphs.

DIS - Data Redundancy
    - update and delete costly
    - All type of No-Sql doesnt fullfil all application need
    - Dont support ACID prop in general & data entry with consistency constraints.

SQL vs NOSQL -> notes(31 page)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


LEC - 16

Types of Database
1. RELATIONAL DATABASE
    - quite popular, known as RDBMS, commonly use SQL for CRUD op
    - store data in discrete table-> joined through FK
    - they are ubiquitous, stable user base, highly optimised for structured data, stronger gurantee of data normalisation, scalabilty issue(Horizontal scaling), data huge-> system complex

2. OBJECT ORIENTED DATABASE(ObjectDB, GemStone)
    - based in OOPS programming (inheritance, object-identity,encapsulation)
    - supports rich system -> structured and collection type.
    - all info is stored as object packages and available at instantly of multiple table
    ADV - data storage and retieval easy
        - can handle complex data relation and more variety data type than RDBMS
        - friendly for real-world problem
        - work with functionality of OOPs
    DIS - high compx -> issue in CRUD slowed
        - no commuinity
        - no views like RDBMS

3. NO-SQL DATABASE
    - non-tabular, varity-> key-value, columanar,grpah etc
    - flexible , scale easy, schema less, adjust dynamically
    - can handle big data, are open source, capability of horizonatal scaling
    - stores data in format other than relation

4. HIERARCHICAL DATABASE
    - data base is based on concrete hierarchy -> emp report to single department
    - schema is tree like, root is parent -> store data with connected links, subdirectory, child records etc
    - each child record can have only parent, each field of recorded data have only one value, retrieval is hard as we have to travese tree
    - can also be used as physical models cause disk storage system inhertly are same to tree-like struc(hierarchical)
    - ADV - ease of use -> one to many organisation of data make traversing easy and fast.
          - due to sepration of table physically-> info can be added or deteletd without affecting entire db, most prog lang provide struc for tree
    - DIS - inflexible nature, one to many is not ideal for complex struc
          - it cannot describe relationship of each child node who have multi parents
          -  top-to-bottom seq searching is time consuming and more storage(redundant)

5. NETWORK DATABASE(intergrated data store(IDS), IDMS, Raima DM, TurbolMAGE)
    - Extension of Hierarchical database
    - child have freedom to associate with multiple parents records
    - organised in graph
    - can handle complex relation, maintaince is tedious
    - M:N links can make retierval slow
    - no community support

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


LEC - 17(CLUSTERING - Replica same dataset on diff server)

- database clustering or replica-set, is process of combining more than one servers or instance connecting a single db.
- sometimes server may not be adequete to handle req, so clustering is needed
- database clustering, sql server clustering, sql clustering are closely related with sql is the lang to manage database info

ADV - Data Reduncancy - needed as we have multiple server
    - Load Balancing - or scalability - bought by clustering, clustering enables a group of servers (a cluster) to share the incoming traffic, preventing any single server from being overwhelmed which takes us to HA
    - High Availability - amount of time a db is considered available

HOW CLUSTERING WORKS
- in clustering arch, all reqs are splits btw computer, so individual user req is executed and produced by no. of computer system.
- clustering is serviceable definitely by ability of load balancing and high-availability. if one nodes collapses, the request is handled by another node. so no possiblities of absolute system failure

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


LEC - 18

Partitioning - big problem can be solved if chopped down in little ones
             - divide a big db, containing data metrices and indexed into smaller and handy slices called partitions
             - these are used by sql without altertation
             - technique used to divide stored db obj into seperate severs, inc perfo, controlability of data
             - horizontally scale server-macines, handle big data easily

Vertical Partitioning - slice column-vise, need to access diff servers to get compelete tuples

Horizontal Partitioning - row-wise, independent chunks of data tuples stored in diff servers

when Partition? database become huge, no of req inc enough to slow down single db server

ADV - Parallelism, Availability, Performance, Managebility, Redice cost, scaling-up verticaly might get costy

Distributed DB - single logical db-> spread accross multi location(servers) and logically connected by network
               - possible -> clustering, Partitioning, Sharding

Sharding - technique for Horizonatal partitoining
         - idea -> instead one db instance, split up and introduce routing layer so that we can forward the req to right instance that actually contain data
         PROs - scalability, availability
         CONS - Complex, not well suited for analytical type queries

**READ CASE STUDY AND LEC 20, 21 FROM NOTES**